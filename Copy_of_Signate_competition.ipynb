{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Signate_competition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzR4MdU3mUu_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "import os\n",
        "# os.environ['KERAS_BACKEND']='theano'\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0X2aI1hmYQ8",
        "outputId": "efe90282-7383-43d2-c3db-2e7a04836d3e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root = '/content/drive/My Drive/Colab Notebooks/'\n",
        "path = root+'Signate/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sZH9lEYmvsE"
      },
      "source": [
        "def clean_str(string):\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    return string.strip().lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMS6O0ivm1c8"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5NPCvonwm4-1",
        "outputId": "0cb3ba3c-c634-4871-89ca-ce510dfbdd33"
      },
      "source": [
        "# reading data\n",
        "# df = pd.read_csv(path+'train.csv', sep='\\t')\n",
        "df = pd.read_csv(path+'training_sn.tsv', sep='\\t')\n",
        "#df.columns =['sid','sentence','html_id','label']\n",
        "#df = df.sample(1000).reset_index()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sid</th>\n",
              "      <th>sentence</th>\n",
              "      <th>html_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tr-01-0000</td>\n",
              "      <td>PART I</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tr-01-0001</td>\n",
              "      <td>The “Business” section and other parts of this...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tr-01-0002</td>\n",
              "      <td>Statements that are not historical facts, inc...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tr-01-0003</td>\n",
              "      <td>Our actual results may differ materially from...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tr-01-0004</td>\n",
              "      <td>Factors that could cause such differences inc...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          sid  ... label\n",
              "0  tr-01-0000  ...     0\n",
              "1  tr-01-0001  ...     0\n",
              "2  tr-01-0002  ...     0\n",
              "3  tr-01-0003  ...     0\n",
              "4  tr-01-0004  ...     0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILx7yy-0oFO0"
      },
      "source": [
        "#df = df.drop(['sentence_cl','Words','sentence_st'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oan3Rc_mpPm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e53cb63-c615-47ae-df2a-cde4a2bb179a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "set(stopwords.words('english'))\n",
        "stop_words = set(stopwords.words('english')) \n",
        "df[\"sentence\"] = df[\"sentence\"].str.strip()\n",
        "df[\"sentence\"] = df[\"sentence\"].str.replace('[^\\w\\s]','')\n",
        "df[\"sentence\"] = df[\"sentence\"].str.lower()\n",
        "\n",
        "#df['sentence'].apply(lambda x: [item for item in x word_tokenize(item)])\n",
        "df['Words']=df['sentence'].apply(lambda x: word_tokenize(x))\n",
        "df['Words_st']=df['Words'].apply(lambda x: [item for item in x if item not in stop_words])\n",
        "#df['Words'] = word_tokenize(df['sentence'])\n",
        "#df[\"Words\"] = df[\"sentence\"].str.lower().str.split()\n",
        "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
        "#df['sentence_st'] = df['sentence'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "#stop = nltk.stopwords.words('english')\n",
        "#print(stop)\n",
        "# Get stopwords, stemmer and lemmatizer\n",
        "#stopwords = nltk.corpus.stopwords.words('english')\n",
        "#stemmer = nltk.stem.PorterStemmer()\n",
        "#lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eXnV0I3T9tBW",
        "outputId": "a0d5defd-d9cd-42d6-a3e8-a0b96b90f39e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sid</th>\n",
              "      <th>sentence</th>\n",
              "      <th>html_id</th>\n",
              "      <th>label</th>\n",
              "      <th>Words</th>\n",
              "      <th>Words_st</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tr-01-0000</td>\n",
              "      <td>part i</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "      <td>[part, i]</td>\n",
              "      <td>[part]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tr-01-0001</td>\n",
              "      <td>the business section and other parts of this f...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "      <td>[the, business, section, and, other, parts, of...</td>\n",
              "      <td>[business, section, parts, form, 10k, contain,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tr-01-0002</td>\n",
              "      <td>statements that are not historical facts inclu...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "      <td>[statements, that, are, not, historical, facts...</td>\n",
              "      <td>[statements, historical, facts, including, sta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tr-01-0003</td>\n",
              "      <td>our actual results may differ materially from ...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "      <td>[our, actual, results, may, differ, materially...</td>\n",
              "      <td>[actual, results, may, differ, materially, pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tr-01-0004</td>\n",
              "      <td>factors that could cause such differences incl...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "      <td>[factors, that, could, cause, such, difference...</td>\n",
              "      <td>[factors, could, cause, differences, include, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          sid  ...                                           Words_st\n",
              "0  tr-01-0000  ...                                             [part]\n",
              "1  tr-01-0001  ...  [business, section, parts, form, 10k, contain,...\n",
              "2  tr-01-0002  ...  [statements, historical, facts, including, sta...\n",
              "3  tr-01-0003  ...  [actual, results, may, differ, materially, pro...\n",
              "4  tr-01-0004  ...  [factors, could, cause, differences, include, ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFaWeDmbBAql",
        "outputId": "de35b2cb-3c84-4ab1-8822-429e6c09d6fe"
      },
      "source": [
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"you're\", 'wasn', 'not', 'how', 'didn', 'at', 'very', 'mustn', 'are', 'herself', 'nor', 'each', 'y', \"shouldn't\", 'mightn', 'this', 'they', 'my', 'be', 'because', \"should've\", 'while', 'shouldn', 'hasn', 's', 'have', 'only', 'yourself', 'all', 'no', 'until', 'aren', 'but', 'doing', 'which', 'haven', 'who', 'isn', 'off', 'further', 'about', \"couldn't\", 'their', 'some', 'his', 'me', 'there', 'shan', \"wouldn't\", 'is', 've', 'her', 'now', 'weren', 'the', 'ain', 'did', 'few', 'just', \"hadn't\", 'couldn', 'she', 'by', 'been', 'yourselves', 'those', 'between', 'above', 'when', 'out', \"weren't\", 'theirs', \"mightn't\", 'hers', 'on', 'ma', 'so', 'it', 'to', \"that'll\", 'them', \"you'll\", \"needn't\", 'too', \"aren't\", \"you'd\", 'i', 'd', 'before', \"you've\", 'doesn', 'hadn', \"she's\", 'you', 'more', 'am', 'than', \"isn't\", 'wouldn', 'that', 'itself', \"didn't\", 'yours', 'same', 'once', 'as', 'then', 'has', \"hasn't\", \"shan't\", \"mustn't\", 'o', 't', 'through', 'a', 'after', 'with', 'why', 'can', 'what', 're', \"wasn't\", 'from', 'our', 'does', 'your', 'against', 'himself', 'if', 'do', 'and', 'needn', 'll', 'we', \"it's\", 'him', 'over', 'for', \"don't\", 'up', 'or', 'being', 'here', 'were', 'an', 'should', 'will', 'in', 'down', 'any', 'below', 'these', 'm', 'such', 'had', 'having', 'where', 'during', \"haven't\", 'won', \"doesn't\", 'whom', 'its', 'he', 'into', 'under', 'was', 'own', 'ours', 'again', 'don', \"won't\", 'of', 'ourselves', 'themselves', 'both', 'other', 'most', 'myself'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9AzumzorxcF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX0veWaCeKwe"
      },
      "source": [
        "\n",
        "#df.shape\n",
        "#df1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH8zmPwle2Rd",
        "outputId": "2af36e85-a0f7-4c40-b652-2d289f24c1de"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(65963, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "_4omEMX-lhAb",
        "outputId": "acbb9ca8-5445-4e92-d022-fcdecc22b3f2"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "stemmer =  nltk.stem.SnowballStemmer('english')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "#df1['Words_st']=df1['Words_st'].apply(lambda x: lemmatizer.lemmatize(x))\n",
        "def lemmatize_text(text):\n",
        "    return [lemmatizer.lemmatize(w) for w in text]\n",
        "def stem_text(text):\n",
        "    return [stemmer.stem(w) for w in text]\n",
        "df['Words'] = df.Words_st.apply(stem_text)    \n",
        "df['Words_lemmatized'] = df.Words_st.apply(lemmatize_text)\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(df['Words_lemmatized'])\n",
        "#df1.drop(['Words_tok'],axis=1)\n",
        "df['Words_len']=df['Words_lemmatized'].apply(lambda x: len(x))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9e0e0453c916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstem_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWords_st\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Words_lemmatized'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWords_st\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatize_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_NB_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-9e0e0453c916>\u001b[0m in \u001b[0;36mstem_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstem_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWords_st\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Words_lemmatized'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWords_st\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatize_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-9e0e0453c916>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstem_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWords_st\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Words_lemmatized'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWords_st\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatize_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'WordNetLemmatizer' object has no attribute 'stem'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAGJ2SSkCiqE"
      },
      "source": [
        "#df['Words_lemmatized'] = pd.Series(df['Words_lemmatized'])\n",
        "#df['Words_lemmatized'] = df.Words_st.apply(lemmatize_text)\n",
        "df1 = df[df['Words_len'] > 3 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "bEfTS__iEvK6",
        "outputId": "c09302e8-2cb1-4cf6-cf4e-d82c6cca8520"
      },
      "source": [
        "\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sid</th>\n",
              "      <th>sentence</th>\n",
              "      <th>html_id</th>\n",
              "      <th>label</th>\n",
              "      <th>Words</th>\n",
              "      <th>Words_st</th>\n",
              "      <th>Words_lemmatized</th>\n",
              "      <th>Words_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tr-01-0001</td>\n",
              "      <td>the business section and other parts of this f...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "      <td>[busi, section, part, form, 10k, contain, forw...</td>\n",
              "      <td>[business, section, parts, form, 10k, contain,...</td>\n",
              "      <td>[business, section, part, form, 10k, contain, ...</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tr-01-0002</td>\n",
              "      <td>statements that are not historical facts inclu...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "      <td>[statement, histor, fact, includ, statement, b...</td>\n",
              "      <td>[statements, historical, facts, including, sta...</td>\n",
              "      <td>[statement, historical, fact, including, state...</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tr-01-0003</td>\n",
              "      <td>our actual results may differ materially from ...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "      <td>[actual, result, may, differ, materi, project,...</td>\n",
              "      <td>[actual, results, may, differ, materially, pro...</td>\n",
              "      <td>[actual, result, may, differ, materially, proj...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tr-01-0004</td>\n",
              "      <td>factors that could cause such differences incl...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "      <td>[factor, could, caus, differ, includ, limit, d...</td>\n",
              "      <td>[factors, could, cause, differences, include, ...</td>\n",
              "      <td>[factor, could, cause, difference, include, li...</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>tr-01-0005</td>\n",
              "      <td>managements narrative analysis of the results ...</td>\n",
              "      <td>Form10k_01</td>\n",
              "      <td>0</td>\n",
              "      <td>[manag, narrat, analysi, result, oper, mna, fo...</td>\n",
              "      <td>[managements, narrative, analysis, results, op...</td>\n",
              "      <td>[management, narrative, analysis, result, oper...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          sid  ... Words_len\n",
              "1  tr-01-0001  ...        12\n",
              "2  tr-01-0002  ...        17\n",
              "3  tr-01-0003  ...        10\n",
              "4  tr-01-0004  ...        19\n",
              "5  tr-01-0005  ...         8\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txQm4P0_9Mh9"
      },
      "source": [
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
        "#txt_fitted = tf.fit(df1[['Words_lemmatized']])\n",
        "#df1['Words_tok'] = txt_fitted.transform(df1[['Words_lemmatized']])\n",
        "#df1['Words_tok']=df1['Words_lemmatized'].apply(lambda x: txt_fitted.transform(x))\n",
        "df1['Words_lemmatized'] = ''.join(str(s) for s in df1['Words_lemmatized'])\n",
        "#df1['Words_tok']=df1['Words_lemmatized'].apply(lambda x: text_to_word_sequence(x))\n",
        "df1['Words_tok']=df1['Words_lemmatized'].apply(lambda x: tokenizer.texts_to_sequences(x))\n",
        "\n",
        "\n",
        "#df1['Words_tok']=df1['Words_lemmatized'].apply(lambda x: tokenizer.(x))\n",
        "#df1['Words_tok'] = pd.Series(df1['Words_tok'])\n",
        "df1.head()\n",
        "\n",
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "#vectorizer = CountVectorizer()\n",
        "#vectorizer.fit(df1[['Words_lemmatized']])\n",
        "#df1['Words_cv'] = vectorizer.transform(df1['Words_lemmatized'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu1ndqVEAEAO",
        "outputId": "23f265b1-4dae-46ce-8179-16a0c8e69cf5"
      },
      "source": [
        "df1['Words_tok'] \n",
        "df1[df1['Words_len']>50]\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "#sns.countplot(x ='Words_len', data = df1)\n",
        "  \n",
        "# Show the plot\n",
        "#plt.show()\n",
        "df1['Words_tok']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1         \n",
              "2         \n",
              "3         \n",
              "4         \n",
              "5         \n",
              "        ..\n",
              "65946     \n",
              "65949     \n",
              "65952     \n",
              "65955     \n",
              "65958     \n",
              "Name: Words_tok, Length: 58316, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6ExX6E4BRy1"
      },
      "source": [
        "df1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgAmvu76Bm0V"
      },
      "source": [
        "df_model = df1[['sid','Words_tok','label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2ztBOs2CDlZ"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "unique_tokens = len(word_index)\n",
        "#understandig purpose only\n",
        "print(word_index)\n",
        "print('Number of Unique Tokens',unique_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKaN0GpuB28W"
      },
      "source": [
        "df_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0kybDNzKFC7"
      },
      "source": [
        "#df_model['Words_pad']=df_model['Words_tok'].apply(lambda x: pad_sequences(x , maxlen=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BadjhJRLMTst"
      },
      "source": [
        "df_model['Words_pad'] = tf.keras.preprocessing.sequence.pad_sequences(df_model['Words_tok'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}